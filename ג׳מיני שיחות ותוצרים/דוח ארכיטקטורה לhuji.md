דוח ארכיטקטורה לhuji

בניית מסגרת AI לאקדמיה בישראלCompletedDec 2, 6:59 PM

פרומפט: אני סטונדט למשפטים שמעוניין לבנות ai framework, לאוניברסיטה שאני לומד בה בישראל.
אין לי רקע טכני, אבל אני רוצה ללמוד את הבסיס ליצירת frameworks, וai govrnacne.
למד אותי את הנושא, והתייחס למקורות עדכניים ביותר נכון להיום דצמבר 2025.



ביצוע מחקר באתרים

(1) חפש מקורות לימוד והסברים על יסודות הבינה המלאכותית (AI) והמודלים הגנרטיביים (Generative AI) המותאמים לאנשים ללא רקע טכני, עם דגש על מחזור חיי פיתוח מערכת (AI Lifecycle). (2) סקור את עקרונות ה-AI Governance והסטנדרטים הבינלאומיים המובילים נכון לסוף שנת 2025, כגון עדכונים ל-EU AI Act, תקני ISO (כדוגמת 42001) ומסגרות העבודה של NIST. (3) חקור את המצב הרגולטורי והמשפטי של תחום הבינה המלאכותית בישראל נכון לדצמבר 2025, כולל הנחיות של משרד המשפטים, רשות החדשנות ומערך הסייבר הלאומי. (4) מצא דוגמאות למדיניות שימוש ונהלי ממשל (Frameworks) של בינה מלאכותית שאומצו באוניברסיטאות ומוסדות אקדמיים בארץ ובעולם. (5) זהה את מרכיבי הליבה הנדרשים לבניית Framework ארגוני: אתיקה, ניהול סיכונים, שקיפות, שמירה על פרטיות וזכויות יוצרים. (6) חפש כלים ומדריכים מעשיים לכתיבת מסמך מדיניות AI המגשר בין הדרישות המשפטיות לבין המגבלות והיכולות הטכנולוגיות. (7) גבש תוכנית למידה ופעולה המשלבת את הידע הטכני הבסיסי עם עקרונות הממשל התאגידי ליצירת מסגרת עבודה מותאמת לאקדמיה.



ניתוח התוצאות



יצירת דוח



יהיה מוכן עוד כמה דקות

# מסגרת אסטרטגית לממשל ורגולציה של בינה מלאכותית באקדמיה הישראלית: דו"ח מחקר מקיף ומדריך יישומי

## תקציר מנהלים

השילוב המואץ של מערכות בינה מלאכותית (AI), ובפרט מודלים של בינה מלאכותית יוצרת (Generative AI), במרחב האקדמי מציב בפני המוסדות להשכלה גבוהה בישראל אתגר כפול: הצורך למנף את הטכנולוגיה לחדשנות מחקרית ופדגוגית, ומנגד, החובה לנהל את הסיכונים המשפטיים, האתיים והתפעוליים הנלווים לה. נכון לדצמבר 2025, הנוף הרגולטורי בישראל ובעולם עבר תמורה דרמטית. כניסתו לתוקף של תיקון 13 לחוק הגנת הפרטיות בישראל באוגוסט 2025, לצד יישום שלבים קריטיים ב-EU AI Act האירופי, הפכו את תחום ה-AI Governance מהמלצה אתית לחובה משפטית וניהולית מחמירה.

דו"ח זה נכתב במיוחד עבור משפטנים ואדמיניסטרטורים באקדמיה, ומטרתו לספק תשתית ידע מעמיקה ופרקטית לבניית מסגרת (Framework) לניהול בינה מלאכותית באוניברסיטה ישראלית. הדו"ח מנתח את ההתפתחויות הרגולטוריות העדכניות ביותר, סוקר את התקנים הבינלאומיים המובילים (ISO 42001, NIST AI RMF), ומציע מודל עבודה סדור המותאם למציאות הישראלית – החל מהקמת ועדות אתיקה, דרך ניהול סיכוני פרטיות וקניין רוחני, ועד לעיצוב מדיניות הוראה המשלבת אוריינות AI. הניתוח מתבסס על לקחים ממוסדות מובילים כגון הטכניון, אוניברסיטת תל אביב וקבוצת ראסל בבריטניה, ומציג מפת דרכים ברורה למעבר מ"כיבוי שריפות" לניהול אסטרטגי יזום.

------

## פרק 1: מבוא – הצורך הדחוף במסגרת ממשל ל-AI באקדמיה

המהפכה הטכנולוגית של השנים האחרונות, שהגיעה לשיאה עם התפוצה הרחבה של מודלי שפה גדולים (LLMs) ומערכות אוטונומיות, שינתה את כללי המשחק באקדמיה. סטודנטים משתמשים בבינה מלאכותית לכתיבת עבודות, חוקרים נעזרים בה לניתוח נתונים וגילוי מדעי, והנהלת האוניברסיטה בוחנת כלים לייעול תהליכים מנהליים. אולם, השימוש הבלתי מבוקר בטכנולוגיות אלו חושף את המוסד לסיכונים משמעותיים.

היעדר מסגרת מסודרת ("Framework") אינו רק בעיה ניהולית, אלא חשיפה משפטית ישירה. כאשר חוקר מזין מידע רפואי רגיש למערכת AI פתוחה, או כאשר מרצה משתמש בכלי AI לבדיקת מבחנים ללא פיקוח אנושי, האוניברסיטה עשויה למצוא את עצמה בהפרה של חוקי הגנת הפרטיות החדשים בישראל, פגיעה בקניין רוחני, ואף חשופה לתביעות בגין אפליה אלגוריתמית. הדו"ח הנוכחי מבקש לגשר על הפער שבין היכולת הטכנולוגית לבין הבקרה המוסדית. הוא מניח את היסודות ליצירת "נמל מבטחים" (Safe Harbor) – סביבה שבה חדשנות יכולה לשגשג בתוך גבולות גזרה ברורים, בטוחים וחוקיים.

### 1.1 הגדרת הבעיה: הפער בין הטכנולוגיה לרגולציה

הקצב המהיר של התפתחות ה-AI, כפי שבא לידי ביטוי בעדכונים תכופים של מודלים (כגון GPT-4 ודומיו), מקדים באופן מסורתי את תהליכי החקיקה והתקינה. אולם, שנת 2025 מסמנת נקודת מפנה. הרגולטורים סגרו את הפער. האיחוד האירופי החיל את ה-AI Act, ארה"ב עדכנה את מסגרות הסיכון של NIST, וישראל הידקה באופן דרמטי את דיני הפרטיות. עבור האוניברסיטה, המשמעות היא שהטיפול ב-AI אינו יכול להישאר עוד ברמת "הנחיות אתיות וולונטריות" אלא חייב לעבור לפסים של ציות (Compliance) וניהול סיכונים מובנה.

------

## פרק 2: הנוף הרגולטורי והמשפטי בשנת 2025

הבסיס לכל מסגרת AI אפקטיבית הוא הבנה עמוקה של דרישות החוק. נכון לדצמבר 2025, אנו פועלים בתוך אקוסיסטם משפטי מורכב המשלב חקיקה מקומית מחמירה עם השפעות בינלאומיות מחייבות.

### 2.1 הזירה הבינלאומית: "אפקט בריסל" וה-EU AI Act

אף על פי שישראל אינה חברה באיחוד האירופי, לרגולציה האירופית יש השפעה מכרעת על מוסדות אקדמיים בישראל, במיוחד אלו המעורבים במחקרים במימון אירופי (כגון תוכניות Horizon) או המשתמשים בתוכנות גלובליות. חוק הבינה המלאכותית האירופי (EU AI Act) נכנס לשלבי יישום קריטיים במהלך 2025, והוא מכתיב סטנדרטים שכל מוסד מחקר רציני חייב להכיר.1

#### לוח הזמנים והמשמעויות (סטטוס 2025)

בפברואר 2025 נכנסו לתוקף האיסורים על מערכות AI המוגדרות כבעלות "סיכון בלתי קביל" (Unacceptable Risk), כגון מערכות לדירוג חברתי או זיהוי ביומטרי בזמן אמת במרחבים ציבוריים ללא אישור מיוחד. באוגוסט 2025, החלו לחול חובות הממשל (Governance) על ספקי מודלים לשימוש כללי (GPAI - General Purpose AI).1

עבור האוניברסיטה, המשמעות היא כפולה:

1. **רכש טכנולוגי:** מערכות שהאוניברסיטה רוכשת מספקים גלובליים (כמו מיקרוסופט, גוגל, Blackboard) עוברות התאמה לרגולציה זו. המסגרת האוניברסיטאית צריכה "לרכוב" על ההתאמות הללו ולא להמציא את הגלגל מחדש.
2. **מחקר משותף:** חוקרים ישראלים המשתפים פעולה עם עמיתים באירופה חייבים להבטיח שהפיתוחים שלהם עומדים בדרישות השקיפות והתיעוד האירופיות, אחרת הם מסתכנים באובדן מימון או ביכולת לפרסם את מחקריהם.

### 2.2 הזירה המקומית: בין רגולציה רכה לאכיפה קשה

מדיניות הממשלה בישראל, כפי שהובלה על ידי משרד החדשנות, המדע והטכנולוגיה ומשרד המשפטים, דגלה באופן מסורתי בגישת "רגולציה רכה" ומאפשרת (Enabling Regulation) כדי לא לפגוע בחדשנות.4 מדיניות זו, המכונה "חדשנות אחראית" (Responsible Innovation), נמנעה מחקיקת "חוק AI" גורף והעדיפה הנחיה של רגולטורים מגזריים. אולם, תמונה זו השתנתה באופן דרמטי באוגוסט 2025 עם כניסתו לתוקף של תיקון 13 לחוק הגנת הפרטיות.

#### 2.2.1 תיקון 13 לחוק הגנת הפרטיות – "שומר הסף" החדש

תיקון 13 מהווה את הרפורמה המשמעותית ביותר בדיני הפרטיות בישראל מזה ארבעה עשורים. הוא משנה את מאזן הכוחות ומשליך ישירות על כל שימוש ב-AI באוניברסיטה המערב מידע אישי.6

| **השינוי בחוק**                   | **המשמעות עבור מסגרת ה-AI באוניברסיטה**                      |
| --------------------------------- | ------------------------------------------------------------ |
| **הרחבת הגדרת "מידע אישי"**       | המונח כולל כעת מזהים דיגיטליים, נתוני מיקום ומידע שנאסף ברשת. מערכות AI המנתחות דפוסי גלישה של סטודנטים או נתוני קמפוס חכם כפופות כעת לחוק. |
| **מידע רגיש במיוחד**              | הגדרה חדשה הכוללת מידע גנטי, ביומטרי, דעות פוליטיות ועוד. שימוש ב-AI למחקר רפואי או סוציולוגי מחייב רמת אבטחה וניהול סיכונים מוגברת. |
| **מינוי ממונה הגנת פרטיות (DPO)** | האוניברסיטה מחויבת למנות DPO בעל סמכויות סטטוטוריות ועצמאות. מסגרת ה-AI חייבת לשלב את ה-DPO בכל תהליך אישור של מערכת חדשה. |
| **סמכויות אכיפה וענישה**          | לרשות להגנת הפרטיות (PPA) יש כעת סמכות להטיל עיצומים כספיים משמעותיים. טעות בניהול הרשאות למערכת AI אינה רק תקלה טכנית, אלא אירוע בעל השלכות פיננסיות ופליליות. |

#### 2.2.2 הנחיות הרשות להגנת הפרטיות לשימוש ב-AI (מאי 2025)

במאי 2025 פרסמה הרשות טיוטת הנחיות ספציפית ליישום חוק הגנת הפרטיות במערכות AI.6 הנחיות אלו הן "מורה הנבוכים" המשפטי עבור האוניברסיטה:

- **בסיס חוקי לאימון מודלים:** הרשות קבעה כי שלב אימון המודל (Training) הוא פעולת עיבוד מידע הדורשת בסיס חוקי נפרד. לא ניתן להסתמך על הסכמה כללית שניתנה למטרה אחרת.
- **חובת השקיפות:** בשימוש בבוטים (Chatbots) מול סטודנטים או מועמדים, חובה ליידע את המשתמש *לפני* האינטראקציה כי הוא משוחח עם מכונה.
- **מנגנוני "רשימות שחורות":** ארגונים נדרשים לנהל מנגנונים שמונעים ממידע שנמחק לחזור למערכת דרך אימון מחדש של המודל – אתגר טכנולוגי שמסגרת ה-AI חייבת לתת לו מענה.8

------

## פרק 3: עקרונות הליבה של מסגרת ה-AI האוניברסיטאית

לפני שיורדים לפרטים הטכניים, על הסטודנט למשפטים להבין את העקרונות הנורמטיביים המנחים את המסגרת. עקרונות אלו נגזרים מהסטנדרטים המובילים בעולם, לרבות ה-OECD AI Principles וה-NIST AI RMF, ומותאמים להקשר הישראלי.4

### 3.1 הוגנות ומניעת אפליה (Fairness)

מערכות AI עלולות לשכפל ואף להעצים הטיות חברתיות הקיימות בנתונים עליהם אומנו. בהקשר של אוניברסיטה ישראלית, הנושא רגיש במיוחד לאור הגיוון האוכלוסייתי (יהודים, ערבים, חרדים, עולים חדשים).

- **האתגר:** מודל לחיזוי נשירה של סטודנטים עלול לסמן באופן שגוי סטודנטים מקבוצות מיעוט כבעלי סיכון גבוה, רק בשל הטיות בנתונים היסטוריים.
- **המענה במסגרת:** חובת ביצוע תסקיר השפעה אלגוריתמי (Algorithmic Impact Assessment) לפני הטמעת כל מערכת המשפיעה על זכויות סטודנטים, כדי לוודא שאין הטיה סטטיסטית כנגד קבוצות מוגנות.10

### 3.2 שקיפות והסברתיות (Transparency & Explainability)

עקרון "הקופסה השחורה" – מצב בו המערכת מקבלת החלטה אך אינה יכולה להסביר מדוע – הוא פסול משפטית ואתית בהקשרים אקדמיים.

- **היישום:** אם סטודנט מקבל ציון או המלצה אקדמית המבוססת על כלי AI, עומדת לו הזכות לקבל הסבר (Meaningful Explanation) על הפרמטרים שהובילו להחלטה. המסגרת חייבת לאסור שימוש במערכות שאינן ניתנות להסבר בהחלטות בעלות השפעה מהותית (High Stakes).11

### 3.3 אחריותיות ופיקוח אנושי (Human-in-the-Loop)

העיקרון המנחה הוא שבינה מלאכותית היא כלי עזר להחלטה (Decision Support), ולא מקבלת ההחלטה הסופית.

- **המנגנון:** כל פלט של מערכת AI המשמש למחקר קליני, מתן ציונים, או קבלת החלטות מנהליות, חייב לעבור אימות על ידי גורם אנושי מוסמך. האוניברסיטה לא יכולה להסתתר מאחורי "האלגוריתם אמר" במקרה של טעות או רשלנות.12

### 3.4 בטיחות ואמינות (Safety & Reliability)

מערכות AI, ובפרט מודלים של שפה (Generative AI), נוטות ל"הזיות" (Hallucinations) – המצאת עובדות בביטחון מלא.

- **הסיכון:** חוקר המסתמך על תקציר מאמר שנוצר על ידי AI ללא בדיקה עלול לפרסם מידע שגוי ולפגוע במוניטין המדעי של המוסד.
- **המענה:** הטמעת נהלי אימות קפדניים והדרכה לשימוש ביקורתי בכלים אלו.

------

## פרק 4: המודל התפעולי – בנית המסגרת הלכה למעשה

חלק זה הוא ה"לב" של הפרויקט עבור הסטודנט. הוא מתרגם את העקרונות התיאורטיים למבנה ארגוני ותהליכי עבודה ברורים. המודל המוצע מבוסס על תקן ISO/IEC 42001 לניהול מערכות בינה מלאכותית, המהווה את הסטנדרט המוביל כיום בעולם לארגון וניהול התחום.13

### 4.1 שלב א': ארכיטקטורת הממשל (Governance Structure)

ניהול AI אינו יכול להיות נחלתה הבלעדית של מחלקת ה-IT. נדרש גוף רב-תחומי.

המלצה להקמת ועדת היגוי ל-AI (AI Steering Committee):

- **יו"ר:** סגן הנשיא למחקר (VPR) או הרקטור – להבטחת סמכות אקדמית.
- **ייעוץ משפטי:** מומחה לדיני פרטיות וקניין רוחני.
- **מנהל טכנולוגיות ראשי (CIO/CISO):** לאבטחת מידע וסייבר.
- **נציג אתיקה:** איש סגל מתחום הפילוסופיה/משפטים (בדומה למודל של מרכז האתיקה באוניברסיטה העברית 16).
- **נציגות סטודנטים:** להבטחת שקיפות ומניעת תחושת "אח הגדול".

**תפקידי הוועדה:** אישור מדיניות, בחינת רכש של מערכות בסיכון גבוה, ועדכון שנתי של המסגרת בהתאם לשינויי רגולציה.

### 4.2 שלב ב': מודל ניהול הסיכונים (Risk Assessment)

לא כל שימוש ב-AI דורש אותו רמת פיקוח. המסגרת צריכה לאמץ גישה מבוססת סיכון (Risk-Based Approach), בדומה ל-EU AI Act.2

#### טבלת סיווג סיכונים לשימוש האוניברסיטה

| **רמת סיכון**            | **הגדרה**                                | **דוגמאות לשימוש**                                           | **דרישות הממשל**                                             |
| ------------------------ | ---------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **אסור (Prohibited)**    | סיכון בלתי קביל לזכויות אדם או אתיקה.    | מעקב ביומטרי אחר נוכחות סטודנטים; ניתוח רגשות בראיונות קבלה; שימוש ב-AI לכתיבת המלצות אקדמיות ללא גילוי. | איסור מוחלט. אין חריגים.                                     |
| **גבוה (High Risk)**     | השפעה על עתיד הסטודנט, בריאות או פרטיות. | עוזרי הוראה מבוססי AI למתן ציונים; צ'אטבוטים לייעוץ פסיכולוגי; סינון מועמדים לקבלה; מחקר רפואי עם מידע גנטי. | מחייב אישור ועדת היגוי; תסקיר השפעה על הפרטיות (DPIA); פיקוח אנושי הדוק; רישום מלא של הפעולות (Logging). |
| **מוגבל (Limited Risk)** | אינטראקציה עם מערכת AI ללא השפעה קריטית. | צ'אטבוט לשאלות נפוצות (שכר לימוד, מפות קמפוס); כלי עזר לחיפוש בספרייה. | חובת שקיפות (יידוע המשתמש שמדובר בבוט); אפשרות לפנייה לנציג אנושי. |
| **מזערי (Minimal Risk)** | כלי עזר לפרודוקטיביות ומחקר בסיסי.       | תיקון דקדוק (Grammarly); סיכום מאמרים (ללא מידע סודי); כתיבת קוד למחקר (בסביבה מבודדת). | שימוש חופשי בכפוף להנחיות כלליות ומודעות לאבטחת מידע.        |

### 4.3 שלב ג': בקרת נתונים ופרטיות (Data Governance)

זהו החלק הקריטי ביותר לצורך עמידה בתיקון 13 לחוק הגנת הפרטיות. המסגרת חייבת להגדיר "אזורים אסורים" ו"אזורים מותרים" למידע.

1. **איסור הזנת מידע רגיש למערכות ציבוריות:** המדיניות חייבת לקבוע במפורש כי חל איסור מוחלט להזין מידע אישי מזוהה, מידע רפואי, או מידע מחקרי סודי למערכות AI חינמיות או ציבוריות (כגון הגרסה החינמית של ChatGPT), שכן מידע זה משמש לאימון המודל ועלול לדלוף.17
2. **רכש מורשה (Walled Garden):** האוניברסיטה צריכה לספק לחוקרים ולסגל גישה לכלים ארגוניים (Enterprise Editions) בהם יש התחייבות חוזית שהמידע לא יוצא החוצה ולא משמש לאימון (למשל Microsoft Copilot ברישיון אוניברסיטאי).13
3. **מחיקת מידע:** יש לוודא כי המערכות הנרכשות תומכות בזכות למחיקה. אם סטודנט דורש למחוק את המידע עליו, האוניברסיטה חייבת להיות מסוגלת לבצע זאת טכנית, בהתאם להנחיות הרשות להגנת הפרטיות.19

### 4.4 שלב ד': תהליכי רכש ושרשרת אספקה

כל רכש של תוכנה חדשה חייב לעבור דרך "שאלון סיכוני AI" (בדומה ל-HECVAT של EDUCAUSE). השאלון יבדוק: האם הספק עומד בתקן ISO 42001? היכן נשמר המידע? האם המודל אומן על מידע שהושג כדין?.20

------

## פרק 5: מדיניות אקדמית ופדגוגית

האוניברסיטה אינה רק תאגיד המנהל סיכונים, אלא מוסד חינוכי. המדיניות כלפי הסטודנטים חייבת לנוע מאיסור גורף לשילוב מושכל ("Responsible Integration"), כפי שמובילים מוסדות כמו סטנפורד וקבוצת ראסל.21

### 5.1 יושרה אקדמית בעידן ה-AI

המסגרת צריכה להגדיר מחדש מהי "העתקה". שימוש ב-AI אינו בהכרח רמאות, אלא תלוי בהקשר.

- **הצהרת שימוש:** סטודנטים יידרשו לצרף לכל עבודה "הצהרת AI" המפרטת באילו כלים השתמשו ולאיזו מטרה (למשל: "השתמשתי ב-Claude לסיעור מוחות וארגון ראשי פרקים, הטקסט נכתב עצמאית").23
- **איסור הסתמכות עיוורת:** המדיניות תקבע כי הסטודנט אחראי באופן מלא לתוכן העבודה, כולל ציטוטים שגויים או מידע כוזב שמקורו ב"הזיה" של המודל.

### 5.2 תבניות לסילבוס (Syllabus Templates)

כדי למנוע בלבול, המסגרת תספק למרצים "תפריט" של שלוש אפשרויות מדיניות, אותן יוכלו לשלב בסילבוס הקורס 23:

1. **שימוש פתוח (AI-First):** "בקורס זה מומלץ להשתמש בכלי AI. אנו נלמד כיצד להנדס פרומפטים (Prompts) ולבקר את התוצרים. חובה לצרף יומן שימוש כנספח לעבודה."
2. **שימוש מותנה/מוגבל:** "ניתן להשתמש ב-AI לשלבי התכנון והעריכה הלשונית בלבד. חל איסור לייצר את הטקסט הסופי באמצעות הכלי."
3. **ללא שימוש (Zero AI):** "בקורס זה, מטרת הלמידה היא פיתוח חשיבה עצמאית וכתיבה ללא סיוע. השימוש ב-AI אסור בהחלט. ייערכו בחינות בעל פה לאימות ידע."

### 5.3 סוגיית כלי הזיהוי (AI Detectors)

המחקרים העדכניים מצביעים על כך שכלי זיהוי AI (כמו Turnitin) סובלים משיעור גבוה של חיוביים-שגויים (False Positives), במיוחד בקרב כותבים שאינם דוברי אנגלית כשפת אם – אוכלוסייה רלוונטית מאוד בישראל.

- **המלצה למדיניות:** המסגרת צריכה להמליץ *שלא* לבסס הליכים משמעתיים אך ורק על תוצאות של כלי זיהוי אוטומטיים. יש להשתמש בהם כאינדיקציה בלבד ולדרוש ראיות נוספות (כגון בחינה בעל פה על החומר).26

------

## פרק 6: יישום טכני ואבטחת מידע

מערך הסייבר הלאומי של ישראל פרסם הנחיות לאבטחת מערכות AI.27 המסגרת האוניברסיטאית צריכה לאמץ הנחיות אלו כסטנדרט טכני.

### 6.1 הגנה מפני התקפות על מודלים

מערכות AI חשופות לסוגים חדשים של התקפות סייבר, כגון:

- **Prompt Injection:** מניפולציה על המודל כדי לגרום לו לעקוף את מנגנוני הבטיחות שלו.
- **Data Poisoning:** החדרת מידע זדוני לנתוני האימון כדי לשבש את תוצאות המודל.

המסגרת תדרוש ממחלקת ה-IT לבצע בדיקות חדירות ייעודיות ל-AI (Red Teaming) עבור כל מערכת קריטית לפני עלייתה לאוויר.29

### 6.2 תיעוד ורישום (Logging)

כדי לעמוד בחובת האחריותיות (Accountability), מערכות ה-AI צריכות לשמור לוגים של הקלטים והפלטים (תוך שמירה על פרטיות). זה חיוני לצורך תחקור במקרה של תלונה על אפליה או טעות במתן ציון.

------

## פרק 7: מקרי בוחן ומודלים לחיקוי

כדי לשכנע את הנהלת האוניברסיטה, כדאי להראות מה עושים אחרים.

- **הטכניון:** הוביל מהלך של שילוב לימודי Generative AI כחובה לכל סטודנט למדעי המחשב, מתוך הבנה שהמקצוע משתנה. הטכניון מדגיש בקוד האתי שלו שמכונה אינה יכולה להחליף אמפתיה ושיפוט מוסרי.30
- **אוניברסיטת תל אביב (TAU):** הקימה מרכז רב-תחומי ל-AI ומדעי הנתונים (TAD) המשלב חוקרים מכל הפקולטות. המרכז מקדם גישה של רגולציה מדורגת ושיתוף פעולה בין רגולטורים.5
- **אוניברסיטת סטנפורד:** הקימה את יוזמת AIMES המאפשרת למרצים לשתף רעיונות כיצד לשלב AI בהוראה תוך הצבת גבולות ברורים, ומספקת מאגר של נוסחי מדיניות לסילבוסים.34

------

## פרק 8: סיכום והמלצות ליישום מיידי

עבור הסטודנט למשפטים, הפרויקט של בניית AI Framework הוא הזדמנות להוביל שינוי מוסדי. להלן מפת הדרכים המומלצת ליישום:

1. **שלב המיפוי:** בצע סקר בקרב סגל וסטודנטים כדי להבין באילו כלים משתמשים כיום ("Shadow AI").
2. **ניסוח המדיניות:** השתמש בתבניות המופיעות בדו"ח זה כדי לנסח טיוטת מדיניות המפרידה בין "שימוש מותר", "שימוש מותנה" ו"שימוש אסור".
3. **הקמת מוקד ידע:** צור דף אינטרנט אוניברסיטאי המרכז את הכלים המאושרים, המדריכים המשפטיים והנחיות האתיקה.
4. **הדרכה:** יזום סדנאות למרצים ולסטודנטים בנושא אוריינות AI ואתיקה, שכן הטכנולוגיה חסרת ערך ללא המיומנות האנושית להשתמש בה באחריות.

בעולם שבו הרגולציה מתהדקת והטכנולוגיה מאיצה, האוניברסיטה חייבת לאמץ גישה פרואקטיבית. מסגרת העבודה המוצעת בדו"ח זה מספקת את הכלים המשפטיים, האתיים והמעשיים לעשות זאת, תוך עמידה בסטנדרטים המחמירים ביותר של שנת 2025.

------

## נספח: מילון מונחים משפטי-טכנולוגי

- **Generative AI (בינה מלאכותית יוצרת):** מערכות המסוגלות ליצור תוכן חדש (טקסט, תמונה, קוד) על בסיס למידה מנתונים קיימים.
- **LLM (מודל שפה גדול):** סוג של AI שאומן על כמויות עצומות של טקסט כדי להבין ולייצר שפה אנושית.
- **Hallucination (הזיה):** מצב בו המודל מייצר בביטחון מידע שגוי לחלוטין.
- **DPIA (תסקיר השפעה על הגנת הפרטיות):** תהליך הערכת סיכונים לפרטיות הנדרש לפני הטמעת טכנולוגיות חדשות הכוללות מידע רגיש.
- **Human-in-the-Loop (אדם בלופ):** מנגנון בקרה המחייב מעורבות אנושית בקבלת החלטות של מערכת אוטונומית.

------

*(הערה: דו"ח זה מבוסס על המידע העדכני ביותר נכון לדצמבר 2025 ומשקף את המצב המשפטי בישראל לאחר כניסת תיקון 13 לחוק הגנת הפרטיות לתוקף.)*





הרעיון הוא יותר במובן הבנייה הטכנית - איך לבנות מה זה ai gorvenance and ai framework and ai orechestarions.

מה לדעתך חסר בדוח שיצרת ביחס לאלו?